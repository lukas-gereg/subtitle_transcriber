{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T14:44:10.942083Z",
     "start_time": "2025-02-01T14:44:05.623642Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m configuration \u001B[38;5;241m=\u001B[39m M2M100Config()\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# Initializing a model (with random weights) from the facebook/m2m100_418M style configuration\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mM2M100Model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfiguration\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Accessing the model configuration\u001B[39;00m\n\u001B[0;32m     15\u001B[0m configuration \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mconfig\n",
      "File \u001B[1;32m~\\hello-world-project\\subtitle_transcriber\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1407\u001B[0m, in \u001B[0;36mM2M100Model.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m   1404\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared \u001B[38;5;241m=\u001B[39m M2M100ScaledWordEmbedding(vocab_size, config\u001B[38;5;241m.\u001B[39md_model, padding_idx, embed_scale\u001B[38;5;241m=\u001B[39membed_scale)\n\u001B[0;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m M2M100Encoder(config, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshared)\n\u001B[1;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder \u001B[38;5;241m=\u001B[39m \u001B[43mM2M100Decoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshared\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m config\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflash_attention_2\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1410\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning_once(\n\u001B[0;32m   1411\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttention with Flash Attention 2 does not support `layer_head_mask`. If you need this feature, please use standard attention.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1412\u001B[0m     )\n",
      "File \u001B[1;32m~\\hello-world-project\\subtitle_transcriber\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:1136\u001B[0m, in \u001B[0;36mM2M100Decoder.__init__\u001B[1;34m(self, config, embed_tokens)\u001B[0m\n\u001B[0;32m   1129\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_tokens\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m=\u001B[39m embed_tokens\u001B[38;5;241m.\u001B[39mweight\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_positions \u001B[38;5;241m=\u001B[39m M2M100SinusoidalPositionalEmbedding(\n\u001B[0;32m   1132\u001B[0m     config\u001B[38;5;241m.\u001B[39mmax_position_embeddings,\n\u001B[0;32m   1133\u001B[0m     config\u001B[38;5;241m.\u001B[39md_model,\n\u001B[0;32m   1134\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_idx,\n\u001B[0;32m   1135\u001B[0m )\n\u001B[1;32m-> 1136\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList([\u001B[43mM2M100DecoderLayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(config\u001B[38;5;241m.\u001B[39mdecoder_layers)])\n\u001B[0;32m   1137\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_flash_attention_2 \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflash_attention_2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_sdpa \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39m_attn_implementation \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msdpa\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\hello-world-project\\subtitle_transcriber\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:674\u001B[0m, in \u001B[0;36mM2M100DecoderLayer.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m    671\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactivation_dropout \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mactivation_dropout\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_attn_layer_norm \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim)\n\u001B[1;32m--> 674\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder_attn \u001B[38;5;241m=\u001B[39m \u001B[43mM2M100_ATTENTION_CLASSES\u001B[49m\u001B[43m[\u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_attn_implementation\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder_attention_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention_dropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_decoder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    680\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder_attn_layer_norm \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLayerNorm(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim)\n\u001B[0;32m    682\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, config\u001B[38;5;241m.\u001B[39mdecoder_ffn_dim)\n",
      "File \u001B[1;32m~\\hello-world-project\\subtitle_transcriber\\.venv\\Lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:216\u001B[0m, in \u001B[0;36mM2M100Attention.__init__\u001B[1;34m(self, embed_dim, num_heads, dropout, is_decoder, bias, is_causal, config)\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mk_proj \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(embed_dim, embed_dim, bias\u001B[38;5;241m=\u001B[39mbias)\n\u001B[0;32m    215\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv_proj \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(embed_dim, embed_dim, bias\u001B[38;5;241m=\u001B[39mbias)\n\u001B[1;32m--> 216\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq_proj \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_proj \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(embed_dim, embed_dim, bias\u001B[38;5;241m=\u001B[39mbias)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:112\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[1;34m(self, in_features, out_features, bias, device, dtype)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mregister_parameter(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbias\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m--> 112\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_parameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\linear.py:118\u001B[0m, in \u001B[0;36mLinear.reset_parameters\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mreset_parameters\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001B[39;00m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001B[39;00m\n\u001B[0;32m    117\u001B[0m     \u001B[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001B[39;00m\n\u001B[1;32m--> 118\u001B[0m     \u001B[43minit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkaiming_uniform_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    119\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    120\u001B[0m         fan_in, _ \u001B[38;5;241m=\u001B[39m init\u001B[38;5;241m.\u001B[39m_calculate_fan_in_and_fan_out(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\init.py:518\u001B[0m, in \u001B[0;36mkaiming_uniform_\u001B[1;34m(tensor, a, mode, nonlinearity, generator)\u001B[0m\n\u001B[0;32m    516\u001B[0m bound \u001B[38;5;241m=\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(\u001B[38;5;241m3.0\u001B[39m) \u001B[38;5;241m*\u001B[39m std  \u001B[38;5;66;03m# Calculate uniform bounds from standard deviation\u001B[39;00m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtensor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muniform_\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43mbound\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbound\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "# Hi, I did not run it, but hopefully it can be a nice way - at least a large step approximation - to the final implementation\n",
    "#enjoy\n",
    "\n",
    "#from networkx.algorithms.threshold import weights_to_creation_sequence\n",
    "import re\n",
    "import torch\n",
    "import srt\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "#my global truth:)\n",
    "checkpoint = \"facebook/m2m100_418M\"\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(checkpoint)\n",
    "#translation is a seq2seq problem\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(checkpoint)\n",
    "tokenizer.src_lang = \"cs\"\n",
    "tokenizer.tgt_lang = \"en\"\n",
    "forced_bos_token_id = tokenizer.lang_code_to_id[\"en\"]\n",
    "\n",
    "def load_srt_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        subtitles = list(srt.parse(file.read()))\n",
    "    return subtitles\n",
    "\n",
    "def preprocess_text(subtitles):\n",
    "    cleaned_subs = {}\n",
    "    for sub in subtitles:\n",
    "        text = sub.content.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        cleaned_subs[sub.index] = {\"start\": sub.start, \"end\": sub.end, \"text\": text}  \n",
    "    return cleaned_subs\n",
    "\n",
    "def tokenize_texts(text_dict):\n",
    "    texts = [entry[\"text\"] for entry in text_dict.values()]\n",
    "    tokenized = tokenizer(texts, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "    max_len = max(len(seq) for seq in tokenized[\"input_ids\"])\n",
    "\n",
    "    # pad\n",
    "    input_ids = [seq + [tokenizer.pad_token_id] * (max_len - len(seq)) for seq in tokenized[\"input_ids\"]]\n",
    "    attention_mask = [[1] * len(seq) + [0] * (max_len - len(seq)) for seq in tokenized[\"input_ids\"]]\n",
    "\n",
    "    tokenized_inputs = {\n",
    "        \"input_ids\": torch.tensor(input_ids),\n",
    "        \"attention_mask\": torch.tensor(attention_mask),\n",
    "    }\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "def translate_texts(tokenized_inputs):\n",
    "    inputs = tokenizer(tokenized_inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=forced_bos_token_id,\n",
    "            max_length=200\n",
    "        )\n",
    "    translated_texts = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "    return translated_texts\n",
    "\n",
    "def format_output(subtitles, translated_texts):\n",
    "    translated_dict = {\n",
    "        sub_id: {\n",
    "            \"original\": subtitles[sub_id][\"text\"],\n",
    "            \"translated\": translated_texts[i],\n",
    "            \"start\": subtitles[sub_id][\"start\"],\n",
    "            \"end\": subtitles[sub_id][\"end\"],\n",
    "        }\n",
    "        for i, sub_id in enumerate(subtitles)\n",
    "    }\n",
    "    return translated_dict\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "\n",
    "    file_path = \"\"\n",
    "    subtitles = load_srt_file(file_path)\n",
    "\n",
    "    cleaned_subs = preprocess_text(subtitles)\n",
    "\n",
    "    tokenized_inputs = tokenize_texts(cleaned_subs)\n",
    "\n",
    "    translated_output = format_output(cleaned_subs, translate_texts(tokenized_inputs))\n",
    "    print(json.dumps(translated_output, indent=2, ensure_ascii=False))\n",
    "    \n",
    "    #if we want we still can save them to SRT\n",
    "    # if we want we can do evaluation metrics with BLEU as\n",
    "    #n-grams: type of (n piece of) subsequent tokens to calculate BLEU metrics\n",
    "    #class transformers.modeling_outputs.Seq2SeqModelOutput"
   ],
   "id": "40ae5c503fc61a9f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
